{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d5146-263a-476f-bed0-0c02791681fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional, RepeatVector\n",
    "from tensorflow.keras.applications import VGG19, InceptionV3, ResNet50, DenseNet201\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textwrap import wrap\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import Lambda\n",
    "from keras.utils import plot_model\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2cfd8-eee1-4995-994e-fc6e7392932b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and preprocess the image data\n",
    "train_images_list = os.listdir('flickr30k_images/flickr30k_images/flickr30k_images/')\n",
    "\n",
    "data = pd.read_csv(\"flickr30k_images/results.csv\", sep=\"|\")\n",
    "data.rename(columns={' comment': 'comment'}, inplace=True)\n",
    "data.rename(columns={' comment_number': 'comment_number'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6178be-8fb8-4273-9c16-bd5a07ebc2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def readImage(path,img_size=224):\n",
    "    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img/255.\n",
    "    \n",
    "    return img\n",
    "\n",
    "def display_images(temp_df):\n",
    "    temp_df = temp_df.reset_index(drop=True)\n",
    "    plt.figure(figsize = (20 , 20))\n",
    "    n = 0\n",
    "    for i in range(15):\n",
    "        n+=1\n",
    "        plt.subplot(5 , 5, n)\n",
    "        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n",
    "        image = readImage(f\"flickr30k_images/flickr30k_images/flickr30k_images/{temp_df['image_name'][i].strip()}\")\n",
    "        plt.imshow(image)\n",
    "        plt.title(\"\\n\".join(wrap(temp_df['comment'][i].strip(), 20)))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc11bb-f80a-4440-89b2-561e752466f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_images(data.sample(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddb765-7ec6-4659-8a59-be98367ce0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    data['comment'] = data['comment'].astype(str)  # Convert all values to strings\n",
    "    data['comment'] = data['comment'].apply(lambda x: x.lower())\n",
    "    data['comment'] = data['comment'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
    "    data['comment'] = data['comment'].apply(lambda x: x.replace(\"\\\\s+\",\" \"))\n",
    "    data['comment'] = data['comment'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
    "    data['comment'] = \"startseq \"+data['comment']+\" endseq\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18491e45-2477-4afb-9e9d-b59c190666ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = text_preprocessing(data)\n",
    "captions = data['comment'].tolist()\n",
    "captions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950d29a-a3f8-4450-b0d3-8ebfb9dd4139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in captions)\n",
    "\n",
    "images = data['image_name'].unique().tolist()\n",
    "nimages = len(images)\n",
    "\n",
    "split_index = round(0.85*nimages)\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "train = data[data['image_name'].isin(train_images)]\n",
    "test = data[data['image_name'].isin(val_images)]\n",
    "\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "tokenizer.texts_to_sequences([captions[1]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb3735-0599-430e-847f-062d14b7f3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = 'flickr30k_images/flickr30k_images/flickr30k_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57346c-c93f-4ca5-8490-8dd04d936650",
   "metadata": {},
   "source": [
    "<h2> VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afe989-cc1f-4893-9fc3-2035fd765a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# ... (previous code remains the same)\n",
    "\n",
    "# VGG16 model\n",
    "vgg_model = VGG16()\n",
    "vgg_fe = Model(inputs=vgg_model.input, outputs=vgg_model.layers[-2].output)\n",
    "\n",
    "img_size = 224\n",
    "vgg_features = {}\n",
    "\n",
    "for image in tqdm(data['image_name'].unique().tolist()):\n",
    "    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    vgg_feature = vgg_fe.predict(img, verbose=0)\n",
    "    vgg_features[image] = vgg_feature\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
    "                 vocab_size, max_length, vgg_features, shuffle=True):\n",
    "        self.vgg_features = vgg_features\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n",
    "        X1, X2, y = self.__get_data(batch)\n",
    "        return (X1, X2), y\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        images = batch[self.X_col].tolist()\n",
    "        for image in images:\n",
    "            feature = self.vgg_features[image][0]\n",
    "            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "        return X1, X2, y\n",
    "\n",
    "input1 = Input(shape=(4096,))\n",
    "input2 = Input(shape=(max_length,))\n",
    "\n",
    "img_features = Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "\n",
    "merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n",
    "sentence_features = LSTM(256)(merged)\n",
    "\n",
    "x = Dropout(0.5)(sentence_features)\n",
    "x = add([x, img_features])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "vgg_caption_model = Model(inputs=[input1, input2], outputs=output)\n",
    "vgg_caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "plot_model(vgg_caption_model)\n",
    "vgg_caption_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8fd7a-4fd4-4d7d-8520-e39d50ebf0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
    "                 vocab_size, max_length, vgg_features, shuffle=True):\n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.vgg_features = vgg_features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n",
    "        X1, X2, y = self.__get_data(batch)\n",
    "        return (X1, X2), y\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        images = batch[self.X_col].tolist()\n",
    "        for image in images:\n",
    "            feature = self.vgg_features[image][0]\n",
    "            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad247b-63b4-42a0-a883-3ef1b7df449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(df=train, X_col='image_name', y_col='comment', batch_size=64, directory=image_path,\n",
    "                                      tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, vgg_features=vgg_features)\n",
    "\n",
    "validation_generator = CustomDataGenerator(df=test, X_col='image_name', y_col='comment', batch_size=64, directory=image_path,\n",
    "                                           tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, vgg_features=vgg_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417868b-f77b-410a-b6d5-91cad5757bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vgg_model.h5\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.2,\n",
    "                                            min_lr=0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53371085-1644-44ad-a96b-937884e01093",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vgg_caption_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, earlystopping, learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54accc6-f0b2-4194-9b0f-8068e7458c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552a70d-65ca-4cc7-a486-06b9139d52ab",
   "metadata": {},
   "source": [
    "<h2> InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be56f4e-9ed1-47e6-8850-d5b282b5d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InceptionV3 model\n",
    "inception_model = InceptionV3()\n",
    "inception_fe = Model(inputs=inception_model.input, outputs=inception_model.layers[-2].output)\n",
    "\n",
    "img_size = 299  # InceptionV3 expects input images of size 299x299\n",
    "inception_features = {}\n",
    "\n",
    "for image in tqdm(data['image_name'].unique().tolist()):\n",
    "    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = img / 255.\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    inception_feature = inception_fe.predict(img, verbose=0)\n",
    "    inception_features[image] = inception_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf1bc9-ef58-48fc-b421-f98a88c21f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
    "                 vocab_size, max_length, inception_features, shuffle=True):\n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.inception_features = inception_features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n",
    "        X1, X2, y = self.__get_data(batch)\n",
    "        return (X1, X2), y\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        images = batch[self.X_col].tolist()\n",
    "        for image in images:\n",
    "            feature = self.inception_features[image][0]\n",
    "            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82bac37-f33f-4236-8904-15585ad34ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(2048,))  # InceptionV3's second-to-last layer has 2048 units\n",
    "input2 = Input(shape=(max_length,))\n",
    "\n",
    "img_features = Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "\n",
    "merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n",
    "sentence_features = LSTM(256)(merged)\n",
    "\n",
    "x = Dropout(0.5)(sentence_features)\n",
    "x = add([x, img_features])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "inception_caption_model = Model(inputs=[input1, input2], outputs=output)\n",
    "inception_caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "plot_model(inception_caption_model)\n",
    "inception_caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e34077-9f40-46a1-b415-2bd8d8c9fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(df=train, X_col='image_name', y_col='comment', batch_size=64, directory=image_path,\n",
    "                                      tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, inception_features=inception_features)\n",
    "\n",
    "validation_generator = CustomDataGenerator(df=test, X_col='image_name', y_col='comment', batch_size=64, directory=image_path,\n",
    "                                           tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, inception_features=inception_features)\n",
    "\n",
    "model_name = \"inception_model.h5\"\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            factor=0.2,\n",
    "                                            min_lr=0.00000001)\n",
    "\n",
    "history = inception_caption_model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint, earlystopping, learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e4d33-5779-485c-9b71-d09849ad2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff511c6c-26f2-4257-8cc8-ffa1ab17a775",
   "metadata": {},
   "source": [
    "<h2> ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a291b3-bb30-4c5d-8388-a6f49726c569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "# Load the pre-trained ResNet50 model\n",
    "resnet_model = ResNet50(weights='imagenet')\n",
    "resnet_model = Model(inputs=resnet_model.input, outputs=resnet_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93f3cb-071f-4234-addd-6292d8ddd6b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "resnet_features = {}\n",
    "\n",
    "for image in tqdm(data['image_name'].unique().tolist()):\n",
    "    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)  # Preprocess the image using ResNet50's preprocessing function\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    resnet_feature = resnet_model.predict(img, verbose=0)\n",
    "    resnet_features[image] = resnet_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326d4d4-0b59-4d39-8233-fae5dc1f5e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Sequence):\n",
    "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
    "                 vocab_size, max_length, resnet_features, shuffle=True):\n",
    "        self.df = df.copy()\n",
    "        self.X_col = X_col\n",
    "        self.y_col = y_col\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.resnet_features = resnet_features\n",
    "        self.shuffle = shuffle\n",
    "        self.n = len(self.df)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n",
    "        X1, X2, y = self.__get_data(batch)\n",
    "        return (X1, X2), y\n",
    "\n",
    "    def __get_data(self, batch):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        images = batch[self.X_col].tolist()\n",
    "        for image in images:\n",
    "            feature = self.resnet_features[image][0]\n",
    "            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n",
    "            for caption in captions:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
    "                    X1.append(feature)\n",
    "                    X2.append(in_seq)\n",
    "                    y.append(out_seq)\n",
    "        X1 = np.array(X1)\n",
    "        X2 = np.array(X2)\n",
    "        y = np.array(y)\n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e92d4-ae32-43d2-bb6d-482934af6848",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Input(shape=(2048,))\n",
    "input2 = Input(shape=(max_length,))\n",
    "\n",
    "img_features = Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "\n",
    "merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n",
    "sentence_features = LSTM(256)(merged)\n",
    "\n",
    "x = Dropout(0.5)(sentence_features)\n",
    "x = add([x, img_features])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "resnet_caption_model = Model(inputs=[input1, input2], outputs=output)\n",
    "resnet_caption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "plot_model(resnet_caption_model)\n",
    "resnet_caption_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b278c6-3090-47bb-a347-7f44efab188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_train_generator = CustomDataGenerator(df=train, X_col='image_name', y_col='comment', batch_size=64, directory=image_path,\n",
    "                                             tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, resnet_features=resnet_features)\n",
    "resnet_validation_generator = CustomDataGenerator(df=test, X_col='image_name', y_col='comment', batch_size=64, directory=image_path,\n",
    "                                                  tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, resnet_features=resnet_features)\n",
    "\n",
    "resnet_model_name = \"resnet_model.h5\"\n",
    "resnet_checkpoint = ModelCheckpoint(resnet_model_name,\n",
    "                                    monitor=\"val_loss\",\n",
    "                                    mode=\"min\",\n",
    "                                    save_best_only=True,\n",
    "                                    verbose=1)\n",
    "\n",
    "resnet_earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "resnet_learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                   patience=3,\n",
    "                                                   verbose=1,\n",
    "                                                   factor=0.2,\n",
    "                                                   min_lr=0.00000001)\n",
    "\n",
    "resnet_history = resnet_caption_model.fit(\n",
    "    resnet_train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=resnet_validation_generator,\n",
    "    callbacks=[resnet_checkpoint, resnet_earlystopping, resnet_learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c679b-7d52-4e9d-a90d-8411ad6c2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ee85e-fb09-46f8-9aba-0a4e0fb383fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb62faee-9b98-47b9-8c5e-e137cdf66b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_reg = 0.01\n",
    "l2_reg = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54722e81-22b6-4651-839a-f872ab618fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_caption_model = load_model('inception_model.h5')\n",
    "\n",
    "for layer in inception_caption_model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = l1_l2(l1=l1_reg, l2=l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55faa026-d902-4cf6-9ff0-142d9b140b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_caption_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d102c35-10f6-42fc-a5a1-6318b0c8fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define regularization parameters\n",
    "l1_reg = 0.01\n",
    "l2_reg = 0.01\n",
    "\n",
    "# Load the Inception model\n",
    "inception_caption_model = load_model('inception_model.h5')\n",
    "\n",
    "# Apply regularization to the model layers\n",
    "for layer in inception_caption_model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer = l1_l2(l1=l1_reg, l2=l2_reg)\n",
    "\n",
    "# Compile the model\n",
    "inception_caption_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749b123-4655-4a8b-bd4c-93e33364848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate captions for each model\n",
    "reg_samples = test.sample(10)\n",
    "\n",
    "\n",
    "reg_samples['vgg_caption'] = reg_samples['image_name'].apply(lambda x: predict_caption(inception_caption_model, os.path.join(image_directory, x), tokenizer, max_length, inception_fe))\n",
    "\n",
    "# Get the original captions from the 'data' dataframe\n",
    "reg_original_captions = data.loc[data['image_name'].isin(reg_samples['image_name']), 'comment'].tolist()\n",
    "\n",
    "# Calculate evaluation metrics for each model\n",
    "reg_metrics = calculate_sequence_metrics(inception_caption_model, reg_samples['vgg_caption'].tolist())\n",
    "\n",
    "print(\"Regularization Metrics:\")\n",
    "print(f\"Accuracy: {reg_metrics[0]:.4f}\")\n",
    "print(f\"Precision: {reg_metrics[1]:.4f}\")\n",
    "print(f\"Recall: {reg_metricsreg_metrics[2]:.4f}\")\n",
    "print(f\"F1 Score: {reg_metrics[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbd1f5-0242-48de-ac3c-3400c22c5a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cdcaa-d512-41de-86a6-d201266956db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8df15d-2a1d-4e63-81d0-0059c8d70cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b9c61-feeb-400f-aaa2-f45b2a5d8b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab5de91-5fa8-42b8-a790-d6f38f748d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
